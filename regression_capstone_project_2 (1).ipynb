{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK13peZw_IeA"
      },
      "source": [
        "## <b> Project Title : Seoul Bike Sharing Demand Prediction </b>\n",
        "**Individual Project:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU0IkXrqLdvL"
      },
      "source": [
        "###**Github Link:**\n",
        "\n",
        "https://github.com/RishiAtWork1/Regression-Capstone-Project-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzKBmJ52qcTy"
      },
      "source": [
        "### **Problem Statement:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ3JnhZ2_J_k"
      },
      "source": [
        "\n",
        "**Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evD2QFDi_edi"
      },
      "source": [
        "### <b> Data Description </b>\n",
        "\n",
        "##### <b> The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.</b>\n",
        "\n",
        "\n",
        "### <b>Attribute Information: </b>\n",
        "\n",
        "* ##### Date : year-month-day\n",
        "* ##### Rented Bike count - Count of bikes rented at each hour\n",
        "* ##### Hour - Hour of the day\n",
        "* ##### Temperature-Temperature in Celsius\n",
        "* ##### Humidity - %\n",
        "* ##### Windspeed - m/s\n",
        "* ##### Visibility - 10m\n",
        "* ##### Dew point temperature - Celsius\n",
        "* ##### Solar radiation - MJ/m2\n",
        "* ##### Rainfall - mm\n",
        "* ##### Snowfall - cm\n",
        "* ##### Seasons - Winter, Spring, Summer, Autumn\n",
        "* ##### Holiday - Holiday/No holiday\n",
        "* ##### Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVzetZyv_ppm"
      },
      "source": [
        "### **Importing the libraries and the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9Sqs46Y_u5A"
      },
      "outputs": [],
      "source": [
        "#let's import the modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiuNuC38AYNJ"
      },
      "source": [
        "###**Mount the drive and import the datset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA14f-JgAV21"
      },
      "outputs": [],
      "source": [
        "#let's mount the google drive for import the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8Bh0-twAfQx"
      },
      "outputs": [],
      "source": [
        "\n",
        "#load and read the seol bike data set from drive\n",
        "bike_df = pd.read_csv('/content/drive/MyDrive/SeoulBikeData.csv',encoding='latin')\n",
        "bike_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi61cbUJDhqD"
      },
      "source": [
        "###**Features description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssDX3f-BDj4L"
      },
      "source": [
        "**Breakdown of Our Features:**\n",
        "\n",
        "**Date** : *The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formating in DD/MM/YYYY, type : str*, we need to convert into datetime format.\n",
        "\n",
        "**Rented Bike Count** : *Number of rented bikes per hour which our dependent variable and we need to predict that, type : int*\n",
        "\n",
        "**Hour**: *The hour of the day, starting from 0-23 it's in a digital time format, type : int, we need to convert it into category data type.*\n",
        "\n",
        "**Temperature(°C)**: *Temperature in Celsius, type : Float*\n",
        "\n",
        "**Humidity(%)**: *Humidity in the air in %, type : int*\n",
        "\n",
        "**Wind speed (m/s)** : *Speed of the wind in m/s, type : Float*\n",
        "\n",
        "**Visibility (10m)**: *Visibility in m, type : int*\n",
        "\n",
        "**Dew point temperature(°C)**: *Temperature at the beggining of the day, type : Float*\n",
        "\n",
        "**Solar Radiation (MJ/m2)**: *Sun contribution, type : Float*\n",
        "\n",
        "**Rainfall(mm)**: *Amount of raining in mm, type : Float*\n",
        "\n",
        "**Snowfall (cm)**: *Amount of snowing in cm, type : Float*\n",
        "\n",
        "**Seasons**: *Season of the year, type : str, there are only 4 season's in data *.\n",
        "\n",
        "**Holiday**: *If the day  is holiday period or not, type: str*\n",
        "\n",
        "**Functioning Day**: *If the day is a Functioning Day or not, type : str*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esxwrhuqqyco"
      },
      "source": [
        "#**Understand the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo27cGzDBJTq"
      },
      "outputs": [],
      "source": [
        "#Getting the shape of dataset with rows and columns\n",
        "print(bike_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_w9A2YTCNLP"
      },
      "outputs": [],
      "source": [
        "#Getting all the columns\n",
        "print(\"Features of the dataset:\")\n",
        "bike_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt7PeOZSCSoK"
      },
      "outputs": [],
      "source": [
        "#check details about the data set\n",
        "bike_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljmPoPDkCYoR"
      },
      "outputs": [],
      "source": [
        "#print the unique value\n",
        "bike_df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpTZYv38CbWf"
      },
      "outputs": [],
      "source": [
        "# checking the null value counts\n",
        "print(bike_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-LyfBNSEMUN"
      },
      "outputs": [],
      "source": [
        "# Checking Duplicate Values\n",
        "value=len(bike_df[bike_df.duplicated()])\n",
        "print(\"The number of duplicate values in the data set is = \",value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8vT0ZJECvFS"
      },
      "outputs": [],
      "source": [
        "#Looking for the description of the dataset to get insights of the data\n",
        "bike_df.describe().astype(int).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvp7OZB2Dc-c"
      },
      "source": [
        "* This Dataset contains **8760 lines** and **14 columns**.\n",
        "* In a day we have **24 hours** and we have **365 days** a year so  **365*24 = 8760**, which represents the number of line in the dataset.means we have the data of whole year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkkjwy8uDLl1"
      },
      "outputs": [],
      "source": [
        "#Rename the complex columns name\n",
        "bike_df=bike_df.rename(columns={'Rented Bike Count':'Rented_Bike_Count','Temperature(°C)':'Temperature','Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'Wind_speed','Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(°C)':'Dew_point_temperature','Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
        "                                'Rainfall(mm)':'Rainfall','Snowfall (cm)':'Snowfall','Functioning Day':'Functioning_Day'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlJpCVtcF2rA"
      },
      "source": [
        "###**Breaking date column**\n",
        "* Spliting Date column which is in string format into year, month, day as a category data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4rhbXCFF6AW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Changing the \"Date\" column into three \"year\",\"month\",\"day\" column\n",
        "bike_df['Date'] = bike_df['Date'].apply(lambda x:\n",
        "                                    dt.datetime.strptime(x,\"%d/%m/%Y\"))\n",
        "bike_df['year'] = bike_df['Date'].dt.year\n",
        "bike_df['month'] = bike_df['Date'].dt.month\n",
        "bike_df['day'] = bike_df['Date'].dt.day_name()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAB7QMSEHOt8"
      },
      "outputs": [],
      "source": [
        "#Observing the pattern of data\n",
        "bike_df.iloc[30:35,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnFszDJZI5EN"
      },
      "outputs": [],
      "source": [
        "\n",
        "#creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "bike_df['weekdays_weekend']=bike_df['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "bike_df=bike_df.drop(columns=['Date','day','year'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPn3xiVsTMen"
      },
      "outputs": [],
      "source": [
        "bike_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4J8bTbNTS0e"
      },
      "outputs": [],
      "source": [
        "#counting the value\n",
        "print(bike_df['Functioning_Day'].value_counts())\n",
        "print(bike_df['weekdays_weekend'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2qnt0f-Jcox"
      },
      "source": [
        "* So we convert the \"date\" column into 3 different column i.e \"year\",\"month\",\"day\".\n",
        "* The \"year\" column is basically contain **from 2017 december to 2018 november i.e. one year** **we don't need the \"year\" column so we drop it**.\n",
        "* The other column \"day\", it contains the each day of the month,we don't need each day of each month data but we need the data about, **if a day is a weekday or a weekend** so we convert it into this format and **drop the \"day\" column**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu5NPrVTtHNV"
      },
      "source": [
        "#**Exploratory data analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfizU4kqLBZR"
      },
      "source": [
        "### **Univariate Analysis**\n",
        "* ***The key objective of Univariate analysis is to simply describe the data to find patterns within the data.***\n",
        "* ***we analyse our dependent variable,A dependent variable is a variable whose value will change depending on the value of another variable.***\n",
        "* ***Our dependent variable is \"Rented Bike Count\" so we need to analyse this column with the other columns by using some visualisation plot.first we analyze the category data type then we proceed with the numerical data type***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXs7WVw-Lh50"
      },
      "source": [
        "####**Analysis of Categorical data by visualization**\n",
        "* ***Month***      \n",
        "* ***weekdays_weekend***\n",
        "* ***Hour***\n",
        "* ***Functioning Day***\n",
        "* ***Season***\n",
        "* ***Holiday***\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#categorical_features\n",
        "categorical_column = ['Hour', 'month', 'weekdays_weekend','Seasons','Holiday','Functioning_Day']"
      ],
      "metadata": {
        "id": "xCJniGML0i2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctFR8vEhKXkV"
      },
      "outputs": [],
      "source": [
        "#Visualization by barchart and boxplot of categorical variables\n",
        "plt.figure(figsize=(15,10))\n",
        "for n,col in enumerate(categorical_column):\n",
        "  plt.subplot(2,3,n+1)\n",
        "  plt.title(col)\n",
        "  sns.barplot(data=bike_df,x=col,y='Rented_Bike_Count')\n",
        "  plt.tight_layout();\n",
        "\n",
        "#getting boxplot for categorical features vs rented bike count\n",
        "plt.figure(figsize=(15,10))\n",
        "for n,col in enumerate(categorical_column):\n",
        "  plt.subplot(2,3,n+1)\n",
        "  plt.title(col)\n",
        "  sns.boxplot(data=bike_df,x=col,y='Rented_Bike_Count');\n",
        "  plt.tight_layout();\n",
        "\n",
        "#getting countplot for hour\n",
        "plt.figure(figsize=(12,8))\n",
        "for n,col in enumerate(['weekdays_weekend','Seasons','Holiday','Functioning_Day']):\n",
        "  plt.subplot(2,2,n+1)\n",
        "  plt.title(col)\n",
        "  sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue=col)\n",
        "  plt.tight_layout();\n",
        "\n",
        "#getting countplot for month\n",
        "plt.figure(figsize=(16,4))\n",
        "for n,col in enumerate(['weekdays_weekend','Seasons','Holiday','Functioning_Day']):\n",
        "  plt.subplot(1,4,n+1)\n",
        "  plt.title(col)\n",
        "  sns.pointplot(data=bike_df,x='month',y='Rented_Bike_Count',hue=col)\n",
        "  plt.tight_layout();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryLns_wtN7Hq"
      },
      "source": [
        "* ***generally people use rented bikes during their working hour from 7am to 9am and 5pm to 9pm.***   \n",
        "\n",
        "* ***the demand of the rented bike is high in May, June, July, August, September as compare to other months.these months are comes inside the summer season.***\n",
        "\n",
        "* ***in the week days(Peak Time: 7 am to 9 am, 5 pm to 9 pm) the demand of the bike higher because of the office.***\n",
        "\n",
        "* **on weekend days, the demand of rented bikes are very low specially in the morning hour but from 4 pm to 8 pm the demand slightly increases.**\n",
        "\n",
        "* ***Peoples use rented bikes mostly on functioning day.***\n",
        "\n",
        "* ***highest rented bike : in summer season(peak time: 7am-9am and 5pm-9pm), lowest rented bike : in winter season, highest rented bike on holidays : in May-June month, lowest rented bike on holidays : in December-January-February month***\n",
        "\n",
        "* ***the use of rented bike is highest when there is no holiday(peak time: 2pm-8pm)***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skDTkFTPL2s4"
      },
      "source": [
        "####**Analysis of Numerical data by visualization**\n",
        "* ***Numerical data is a data type expressed in numbers, rather than natural language description. Sometimes called quantitative data, numerical data is always collected in number form. Numerical data differentiates itself from other number form data types with its ability to carry out arithmetic operations with these numbers.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B11zZ5F0Vfs6"
      },
      "outputs": [],
      "source": [
        "#making list of continuous variables\n",
        "numerical_features = ['Temperature', 'Humidity', 'Wind_speed',\n",
        "       'Visibility', 'Dew_point_temperature', 'Solar_Radiation', 'Rainfall',\n",
        "       'Snowfall']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6JJr5z4WDV_"
      },
      "source": [
        "###**Numerical vs rented_bike_count**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dvdyNpxXHxC"
      },
      "outputs": [],
      "source": [
        "# making function to print all numerical features\n",
        "plt.figure(figsize=(16,8))\n",
        "for n,col in enumerate(numerical_features):\n",
        "  plt.subplot(2,4,n+1)\n",
        "  bike_df.groupby(col).mean()['Rented_Bike_Count'].plot();\n",
        "  plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sywxGdTgbi8G"
      },
      "source": [
        "* From the above plot we see that people like to ride bikes when it is pretty hot around 25°C average.\n",
        "* We can see from the above plot that the **demand of rented bike is uniformly distributed** **despite of wind speed** but when the speed of wind was 7 m/s then the demand of bike also increase that clearly means **peoples love to ride bikes when its little windy**.\n",
        "* from the above plot we see that, the **amount of rented bikes is huge, when there is solar radiation**. when there is increase in solar radiation, there is increase in rented bike.\n",
        "* we can see from above plot, **when there is no rain, rented bike count is huge**, but when there is rain, we can see increase in bike rented only at certain points, maybe it is when people are going to home from office on weekdays. but otherwise,there is no huge spike in the rented bike.\n",
        "* We can see from the plot that, on the y-axis, **the amount of rented bike is very low When we have more than 4 cm of snow**, the bike rents is much lower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHcUpksagpyj"
      },
      "source": [
        "###**Regression plot**\n",
        "**The regression plots in seaborn are primarily intended to add a visual guide that helps to emphasize patterns in a dataset during exploratory data analyses. Regression plots as the name suggests creates a regression line between 2 parameters and helps to visualize their linear relationships.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VixdzzwQbPdp"
      },
      "outputs": [],
      "source": [
        "#printing the regression plot for all the numerical features\n",
        "plt.figure(figsize=(16,8))\n",
        "for n,col in enumerate(numerical_features):\n",
        "  plt.subplot(2,4,n+1)\n",
        "  sns.regplot(x=bike_df[col],y=bike_df['Rented_Bike_Count'],scatter_kws={\"color\": 'orange'}, line_kws={\"color\": \"black\"});\n",
        "  plt.title(col)\n",
        "  plt.tight_layout();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia4ScPJ3hU24"
      },
      "source": [
        "* From the above regression plot of all numerical features we see that the columns  **Temperature, Wind_speed, Solar_Radiation Snowfall are positively relation to the target variable**.which means the rented bike count increases with increase of these features.\n",
        "* **Rainfall, Snowfall, Humidity these features are negatively related with the target variable** which means the rented bike count decreases when these features increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8itPPycpi5Be"
      },
      "source": [
        "###**Checking skewness and outliers of the continuous Variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j9E_Ax0hWd"
      },
      "outputs": [],
      "source": [
        "#checking distribution and boxplot of continuous features\n",
        "plt.figure(figsize=(16,8))\n",
        "for n,col in enumerate(numerical_features):\n",
        "  plt.subplot(2,4,n+1)\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Density')\n",
        "  ax=sns.distplot(np.sqrt(bike_df[col]),hist=True ,color=\"y\")\n",
        "  ax.axvline((bike_df[col]).mean(),color='red')\n",
        "  ax.axvline((bike_df[col]).median(),color='black')\n",
        "  plt.title(col)\n",
        "  plt.tight_layout();\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "for n,col in enumerate(numerical_features):\n",
        "    plt.subplot(2,4,n+1)\n",
        "    plt.ylabel(col)\n",
        "    sns.boxplot(x=(bike_df[col]))\n",
        "    plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQvyAAwkjMPg"
      },
      "source": [
        "* The above graph shows that Rented_Bike_Count,Visibility,Solar_Radiation,Rainfall,Snowfall has **highly skewed**. Since the assumption of linear regression is that 'the **distribution of dependent and independent variable has to be normal**', so we should perform some operation to make it normal.\n",
        "* From boxplot, we have detected outliers in Rented Bike Count, Snowfall,Rainfall, Solar radiation, Wind speed column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVlKW5dJ2YMk"
      },
      "source": [
        "### **Methods to treat outliers:**\n",
        "* **Trimming/Remove the outliers:** In this technique, we remove the outliers from the dataset. Although it is not a good practice to follow.\n",
        "* **Quantile based flooring and capping:** In this technique, the outlier is capped at a certain value above the 90th percentile value or floored at a factor below the 10th percentile value.\n",
        "* **Mean/Median imputation:** As the mean value is highly influenced by the outliers, it is advised to replace the outliers with the median value.\n",
        "\n",
        "###**Method to treat Skewness:**\n",
        "* **1.log transformation:** transform skewed distribution to a normal distribution.\n",
        "* **2.Remove outliers**\n",
        "* **3.Normalize (min-max)**\n",
        "* **4.Cube root:** when values are too large. Can be applied on negative values.\n",
        "* **5.Square root:** applied only to positive values.\n",
        "* **6.Reciprocal**\n",
        "* **7.Square: apply on left skew**\n",
        "* **Skewness in target variable:** Use undersampling, oversampling or SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epoMbdWp2Q1v"
      },
      "outputs": [],
      "source": [
        "#Handling Skewness in the data\n",
        "bike_df['Rented_Bike_Count']=np.sqrt(bike_df['Rented_Bike_Count'])\n",
        "bike_df['Snowfall']=np.cbrt(bike_df['Snowfall'])\n",
        "bike_df['Rainfall']=np.cbrt(bike_df['Rainfall'])\n",
        "bike_df['Solar_Radiation']=np.cbrt(bike_df['Solar_Radiation'])\n",
        "bike_df['Humidity']=np.sqrt(bike_df['Humidity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjrhptKP2RNX"
      },
      "outputs": [],
      "source": [
        "#checking distribution and boxplot of continuous features after applying some algorithm\n",
        "plt.figure(figsize=(16,8))\n",
        "for n,col in enumerate(numerical_features):\n",
        "  plt.subplot(2,4,n+1)\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Density')\n",
        "  ax=sns.distplot(np.sqrt(bike_df[col]),hist=True ,color=\"y\")\n",
        "  ax.axvline((bike_df[col]).mean(),color='red')\n",
        "  ax.axvline((bike_df[col]).median(),color='black')\n",
        "  plt.title(col)\n",
        "  plt.tight_layout();\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "for n,col in enumerate(numerical_features):\n",
        "    plt.subplot(2,4,n+1)\n",
        "    plt.ylabel(col)\n",
        "    sns.boxplot(x=(bike_df[col]))\n",
        "    plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rmJDQpZnjCX"
      },
      "source": [
        "* Since we have generic rule of applying Square root for the skewed variable in order to make it normal .**After applying Square root to the skewed Rented Bike Count, here we get almost normal distribution**.\n",
        "* After applying Square root to the Rented Bike Count column, we find that there is no outliers present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N_m7rYftXCs"
      },
      "source": [
        "#**Feature selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0gV5GKF7-77"
      },
      "source": [
        "###**Correlation Map**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBocHRkk7jfJ"
      },
      "outputs": [],
      "source": [
        "# correlation map\n",
        "plt.figure(figsize=(16,15))\n",
        "corr = bike_df.corr()\n",
        "sns.heatmap(corr,annot=True,fmt='.2f',annot_kws={'size':15})\n",
        "plt.title('correlation between features');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YtavvNQ8MQR"
      },
      "source": [
        "***We can observe on the heatmap that on the target variable line the most positively correlated variables to the bike rent are :***\n",
        "\n",
        "* the temperature\n",
        "* the dew point temperature\n",
        "* the solar radiation\n",
        "* Hour\n",
        "\n",
        "***And most negatively correlated variables are:***\n",
        "* Humidity\n",
        "* Rainfall\n",
        "* weekdays or weekends\n",
        "\n",
        "\n",
        "from above correlation map, we can see that, there is **high correlation between 'Dew Point Temperature' and Temperature**. here we are featuring the best suitable model,sp we have to drop either one of the feature i.e. either Temperature or Dew Point Temperature.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzDf1ZF_b1Mf"
      },
      "source": [
        "###**Checking Multicollinearity using VIF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkQRs2MPj9QV"
      },
      "outputs": [],
      "source": [
        "#Defining Variance Inflaition Factor\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def cal_vif(x):\n",
        "  vif = pd.DataFrame()\n",
        "  vif['variables'] = x.columns\n",
        "  vif['VIF'] = [variance_inflation_factor(x.values,i) for i in range(x.shape[1])]\n",
        "  return vif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFY62XA5kAbr"
      },
      "outputs": [],
      "source": [
        "#Checking Variance Inflaition Factor\n",
        "VIF_table = cal_vif(bike_df[[i for i in bike_df.describe().columns if i not in ['Rented_Bike_Count']]])\n",
        "VIF_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiDrNzYHcFHu"
      },
      "source": [
        "**1. Variance Inflation Factor(VIF)**\n",
        "\n",
        "– If VIF=1; No multicollinearity\n",
        "\n",
        "– If VIF=<5; Low multicollinearity or moderately correlated\n",
        "\n",
        "– If VIF=>5; High multicollinearity or highly correlated\n",
        "\n",
        "**2. Tolerance(Reciprocal of VIF)**\n",
        "\n",
        "– If VIF is high then tolerance will be low i.e, high multicollinearity.\n",
        "\n",
        "– If VIF is low the tolerance will be high i.e, low multicollinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb9xL9Ok7jW6"
      },
      "outputs": [],
      "source": [
        "# dropping dew point temperature column which has high VIF\n",
        "bike_df = bike_df.drop(['Dew_point_temperature'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c6xfQghNOnR"
      },
      "outputs": [],
      "source": [
        "#Checking Variance Inflaition Factor\n",
        "VIF_table = cal_vif(bike_df[[i for i in bike_df.describe().columns if i not in ['Rented_Bike_Count']]])\n",
        "VIF_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8Ok6dduOLly"
      },
      "outputs": [],
      "source": [
        "# dropping dew point temperature column which has high VIF\n",
        "bike_df = bike_df.drop(['Humidity'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing visibility and dewpoint temperature from numerical feature column\n",
        "numerical_features.remove('Dew_point_temperature')\n",
        "numerical_features.remove('Humidity')"
      ],
      "metadata": {
        "id": "_JWZP7HMuO3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZvbrYkGOOqH"
      },
      "outputs": [],
      "source": [
        "#Checking Variance Inflaition Factor\n",
        "VIF_table = cal_vif(bike_df[[i for i in bike_df.describe().columns if i not in ['Rented_Bike_Count']]])\n",
        "VIF_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZLsqe7mOWv1"
      },
      "source": [
        "Since VIF of all variables is <5, we can go with remaining variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBUF7B0K8dGz"
      },
      "outputs": [],
      "source": [
        "#Getting columns\n",
        "bike_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKCTtKk28c-2"
      },
      "outputs": [],
      "source": [
        "# description of numerical values\n",
        "bike_df.describe().astype(float).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP5XV8am8s8C"
      },
      "outputs": [],
      "source": [
        "# description of object values\n",
        "bike_df.describe(include='O').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTZhgT9n80SQ"
      },
      "source": [
        "* here we can see that, **highest rented bike** is during **Spring season**.\n",
        "* here we can see that, **highest rented bike** is when there is **no Holiday and Functioning Day**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjLVv8iztslE"
      },
      "source": [
        "#**Data preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOaDjWzEp2r0"
      },
      "source": [
        "###**Create Dummy Variables**\n",
        "A dataset may contain various type of values, sometimes it consists of categorical values. So, in-order to use those categorical value for programming efficiently we create dummy variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isO0qhP3mp6s"
      },
      "outputs": [],
      "source": [
        "categorical_features = ['Hour','Seasons', 'Holiday', 'Functioning_Day', 'month','weekdays_weekend']\n",
        "categorical_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXGSc6C0qi9r"
      },
      "source": [
        "###**one hot encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbtgje7XtYSP"
      },
      "outputs": [],
      "source": [
        "#one hot encoding using simple method\n",
        "bike_df1 = pd.get_dummies(bike_df,columns=['Hour','Seasons', 'Holiday', 'Functioning_Day', 'month','weekdays_weekend'],drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2Db_C7u1bE1"
      },
      "outputs": [],
      "source": [
        "bike_df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAvbjGVbz6hI"
      },
      "source": [
        "#**ML model implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvfEwwvS0EZr"
      },
      "source": [
        "###**Train Test Split for regression**\n",
        "Before, fitting any model it is a rule of thumb to split the dataset into a training and test set. This means some proportions of the data will go into training the model and some portion will be used to evaluate how our model is performing on any unseen data. The proportions may vary from 60:40, 70:30, 75:25 depending on the person but mostly used is 80:20 for training and testing respectively. In this step we will split our data into training and testing set using scikit learn library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SC3_Vb9w-NK"
      },
      "outputs": [],
      "source": [
        "#Assign the value in X and Y\n",
        "x = bike_df1.drop(['Rented_Bike_Count'],axis=1)\n",
        "y = np.sqrt(bike_df1['Rented_Bike_Count'])\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vHQRW0SXyZm"
      },
      "outputs": [],
      "source": [
        "#Name of Features\n",
        "x_columns = x.columns\n",
        "x_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ulauJbq0zNh"
      },
      "outputs": [],
      "source": [
        "x.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVC5NXDl1npQ"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmGb0RMiPwNN"
      },
      "source": [
        "##**Standardising the features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QP9b48O1poH"
      },
      "outputs": [],
      "source": [
        "#Create test and train data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtMc3BQ4P5oi"
      },
      "outputs": [],
      "source": [
        "#Standardising the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLf0oDNQWQqC"
      },
      "outputs": [],
      "source": [
        "#Transformed data\n",
        "x_train = pd.DataFrame(x_train,columns=x_columns)\n",
        "x_test = pd.DataFrame(x_test,columns=x_columns)\n",
        "print(x_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYxF4MwO12jB"
      },
      "outputs": [],
      "source": [
        "bike_df1.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffw1VFRrg8vz"
      },
      "source": [
        "###**Function for Graphical representation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQpbrX4Lg77h"
      },
      "outputs": [],
      "source": [
        "#function for feature importance\n",
        "def feature_importance(model):\n",
        "    try:\n",
        "      importance = model.feature_importances_\n",
        "      feature = x_columns\n",
        "    except:\n",
        "      importance = np.abs(model.coef_)\n",
        "      feature = x_columns\n",
        "    indices = np.argsort(importance)\n",
        "    indices = indices[20::-1]\n",
        "\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.barh(range(len(indices)),importance[indices] )\n",
        "    plt.yticks(range(len(indices)),[feature[i] for i in indices])\n",
        "    plt.title('Feature Importance')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**function for metric values**"
      ],
      "metadata": {
        "id": "9um0M7KjgMbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metric_value(model,x_train,y_train,x_test,y_test):\n",
        "    #get the y predicted value for train and test dataset\n",
        "    y_pred_train = model.predict(x_train)\n",
        "    y_pred_test = model.predict(x_test)\n",
        "\n",
        "    #Calculating metric value for testing dataset\n",
        "    #Calculate MSE\n",
        "    MSE = mean_squared_error(y_train, y_pred_train)\n",
        "    #calculate RMSE\n",
        "    RMSE = np.sqrt(MSE)\n",
        "    #calculate MAE\n",
        "    MAE = mean_absolute_error(y_train, y_pred_train)\n",
        "    #calculate r2 and adjusted r2\n",
        "    r2 = r2_score(y_train, y_pred_train)\n",
        "    Adjusted_r2 = 1-(((1-r2)*(x_test.shape[0]-1))/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "    #creating table of metric values\n",
        "    data_set = [['MAE',round((MAE),3)],['MSE',round((MSE),3)],\n",
        "                ['RMSE',round((RMSE),3)],['R2_score',round((r2),3)],['Adjusted R2',round((Adjusted_r2),3)]]\n",
        "    training_df = pd.DataFrame(data_set, columns=['metrics','train_values'])\n",
        "\n",
        "    #Calculating metric value for testing dataset\n",
        "    #calculate MSE\n",
        "    MSE = mean_squared_error(y_test, y_pred_test)\n",
        "    #calculate RMSE\n",
        "    RMSE = np.sqrt(MSE)\n",
        "    #calculate MAE\n",
        "    MAE = mean_absolute_error(y_test, y_pred_test)\n",
        "    #calculate r2 and adjusted r2\n",
        "    r2 = r2_score(y_test, y_pred_test)\n",
        "    Adjusted_r2 = 1-(((1-r2)*(x_test.shape[0]-1))/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "    #creating table of metric values\n",
        "    data_set = [['MAE',round((MAE),3)],['MSE',round((MSE),3)],\n",
        "                ['RMSE',round((RMSE),3)],['R2_score',round((r2),3)],['Adjusted R2',round((Adjusted_r2),3)]]\n",
        "    testing_df = pd.DataFrame(data_set, columns=['metrics','test_values'])\n",
        "\n",
        "    metric_values = training_df.merge(testing_df,how='inner', on='metrics')\n",
        "    print(metric_values)\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    ax = metric_values.plot(kind='bar', x='metrics', rot=0)\n",
        "    plt.title(model)\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.show();\n",
        "\n",
        "    ### Heteroscadacity\n",
        "    residuals = y_test - y_pred_test\n",
        "    plt.figure(figsize=(9,3))\n",
        "    #plotting the distribution\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.xlabel('residuals')\n",
        "    sns.distplot(residuals);\n",
        "\n",
        "    #plotting the scatterplot\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.xlabel('scatterplot of residuals')\n",
        "    plt.scatter(y_pred_test,residuals)\n",
        "    plt.tight_layout();\n",
        "\n",
        "    plt.figure(figsize=(12,3))\n",
        "    plt.plot((y_pred_test)[:100])\n",
        "    plt.plot((np.array(y_test)[:100]))\n",
        "    plt.legend([\"Predicted\",\"Actual\"])\n",
        "    plt.title('Actual and Predicted Bike Counts')\n",
        "    plt.show();\n",
        "\n",
        "    try:\n",
        "      if model == xgb_model:\n",
        "        result=pd.DataFrame()\n",
        "        model = [Linear_Regression,ridge_regression,model,rf_model,gradient_boosting_regressor,xgb_model]\n",
        "        for i in model:\n",
        "          result = result.append(metric_values[i],ignore_index=True)\n",
        "        print(result)\n",
        "    except:\n",
        "      pass\n"
      ],
      "metadata": {
        "id": "hNx3RqgOgL-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaLgFDw228TU"
      },
      "source": [
        "##**Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dFwV1041-Lr"
      },
      "outputs": [],
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import LinearRegression\n",
        "Linear_Regression = LinearRegression()\n",
        "Linear_Regression.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the metric value\n",
        "metric_value(Linear_Regression,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "foDbujfvjqlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature importance\n",
        "feature_importance(Linear_Regression)"
      ],
      "metadata": {
        "id": "2UZV5gWZTm6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLRuLlWu55Po"
      },
      "source": [
        "* **Simple Linear Regression Model**\n",
        "* **R2 score of Train data : 0.861**\n",
        "* **adjusted R2 score of Train data : 0.857**\n",
        "* **R2 score of Test data : 0.868**\n",
        "* **adjusted R2 score of Test data : 0.864**\n",
        "* **Performance of data: Better**\n",
        "* To satisfy the regression assumptions and be able to trust the results, **the residuals should have a constant variance**.\n",
        "* **Important features:** weekdays_weekend,holiday_noholiday,hour:6,13,14,12,19,20,15,22,8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIexTjdUFnVZ"
      },
      "source": [
        "##**Ridge Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOJDtBzZhmKD"
      },
      "outputs": [],
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge_regression = Ridge(alpha=0.1)\n",
        "ridge_regression.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the metric value\n",
        "metric_value(ridge_regression,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "4YOZAS268w3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AihPqs1BSTzV"
      },
      "outputs": [],
      "source": [
        "#feature importance\n",
        "feature_importance(ridge_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXuys9hzlpOd"
      },
      "source": [
        "* **Ridge Regression Model**\n",
        "* **R2 score of Train data : 0.861**\n",
        "* **adjusted R2 score of Train data : 0.857**\n",
        "* **R2 score of Test data : 0.867**\n",
        "* **adjusted R2 score of Test data : 0.864**\n",
        "* **Performance of data: Better**\n",
        "* To satisfy the regression assumptions and be able to trust the results, **the residuals should have a constant variance**.\n",
        "* **Important features:** weekdays_weekend,holiday_noholiday,hour:1,16,9, month:3,5,8,9,12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4rmDSr6Fq4j"
      },
      "source": [
        "##**Decision Tree with gridsearchcv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRefrR-49-Lf"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeRegressor(random_state=0)\n",
        "dt_params = {'max_depth':np.arange(1,50,2),'min_samples_leaf':np.arange(2,15)}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "gs_dt = GridSearchCV(dt,dt_params,cv=3)\n",
        "gs_dt.fit(x_train,y_train)\n",
        "a = gs_dt.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_PraHGAnKKe"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dtr = DecisionTreeRegressor(max_depth=a['max_depth'],min_samples_leaf= a['min_samples_leaf'])\n",
        "model = dtr.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the metric value\n",
        "metric_value(model,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "UWDueW6S9Aya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEUGMEYfSY51"
      },
      "outputs": [],
      "source": [
        "#feature importance\n",
        "feature_importance(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1FCt_cuoqM_"
      },
      "source": [
        "* **Decision Tree Regression with GridSearchCV Model**\n",
        "* **R2 score of Train data : 0.98**\n",
        "* **adjusted R2 score of Train data : 0.98**\n",
        "* **R2 score of Test data : 0.89**\n",
        "* **adjusted R2 score of Test data : 0.89**\n",
        "* **Performance of data: Better**\n",
        "* To satisfy the regression assumptions and be able to trust the results, **the residuals should have a constant variance**.\n",
        "* **Important features:** hour:6,13,10, month: 3,12,snowfall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx3KZnTXFx9n"
      },
      "source": [
        "##**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE3tyTU8-5pT"
      },
      "outputs": [],
      "source": [
        "#import the packages\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Create an instance of the RandomForestRegressor\n",
        "rf_model = RandomForestRegressor()\n",
        "rf_model.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the metric value\n",
        "metric_value(rf_model,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "f-YxqIOp9RFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6wbdGjzSbyC"
      },
      "outputs": [],
      "source": [
        "#feature importance\n",
        "feature_importance(rf_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak1qnudorx2s"
      },
      "source": [
        "* **Random Forest Model**\n",
        "* **R2 score of Train data : 0.99**\n",
        "* **adjusted R2 score of Train data : 0.99**\n",
        "* **R2 score of Test data : 0.934**\n",
        "* **adjusted R2 score of Test data : 0.933**\n",
        "* **Performance of data: Better**\n",
        "* To satisfy the regression assumptions and be able to trust the results, **the residuals should have a constant variance**.\n",
        "* **Important features:** hour:10,16,23, month:3,10, snowfall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjgP_921tCxv"
      },
      "source": [
        "Here, we can see that, **Functioning Day, Temperature, and Humidity** has much **higher relevance** on the counting of bike Renting. means they are most important features which affects on rented bike count in the **Random Forest model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F3WeauuF2sf"
      },
      "source": [
        "##**Gradient Boosting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-n9aZeeRreC"
      },
      "outputs": [],
      "source": [
        "#import the packages\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gradient_boosting_regressor = GradientBoostingRegressor()\n",
        "gradient_boosting_regressor.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the metric value\n",
        "metric_value(gradient_boosting_regressor,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "nJMb2H4Y-D9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VeRuSUcSdJe"
      },
      "outputs": [],
      "source": [
        "#feature importance\n",
        "feature_importance(gradient_boosting_regressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbZNSreYw7Sl"
      },
      "source": [
        "* **Gradient Boosting Model**\n",
        "* **R2 score of Train data : 0.91**\n",
        "* **adjusted R2 score of Train data : 0.90**\n",
        "* **R2 score of Test data : 0.90**\n",
        "* **adjusted R2 score of Test data : 0.90**\n",
        "* **Performance of data: Better**\n",
        "* To satisfy the regression assumptions and be able to trust the results, **the residuals should have a constant variance**.\n",
        "* **Important features:** hour:1,7,11, month:8,12, windspeed, season_summer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ic633gaGtp4"
      },
      "source": [
        "###**Gradient Boosting with GridSearchCV**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYgLNtax5aS"
      },
      "source": [
        "**Provide the range of values for chosen hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btBL_fJ3xu_D"
      },
      "outputs": [],
      "source": [
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8,10]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [30,40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDYAuzvPxu1v"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "# Grid search\n",
        "gradient_boosting_gridsearchcv = GridSearchCV(estimator=gb_model,param_grid = param_dict,cv = 5)\n",
        "gradient_boosting_gridsearchcv.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deA99U7X8KUj"
      },
      "outputs": [],
      "source": [
        "gb_optimal_model = gradient_boosting_gridsearchcv.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the metric value\n",
        "metric_value(gb_optimal_model,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "7_urwD_v-UrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FegIynYhSe9x"
      },
      "outputs": [],
      "source": [
        "#feature importance\n",
        "feature_importance(gb_optimal_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mshMjGL4psh3"
      },
      "source": [
        "* **Gradient Boosting with GridSearchCV Model**\n",
        "* **R2 score of Train data : 0.89**\n",
        "* **adjusted R2 score of Train data : 0.89**\n",
        "* **R2 score of Test data : 0.88**\n",
        "* **adjusted R2 score of Test data : 0.88**\n",
        "* **Performance of data: Best**\n",
        "* To satisfy the regression assumptions and be able to trust the results, **the residuals should have a constant variance**.\n",
        "* **Important features:** hour:10,23, month:3,6,10,12, snowfall, holiday_no holiday"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivGiPxijimyp"
      },
      "source": [
        "\n",
        "\n",
        "##**XgBoost with RandomizedSearchCV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B_uRl6ndAbj"
      },
      "outputs": [],
      "source": [
        "#Import Library\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnrC7wlsabWx"
      },
      "outputs": [],
      "source": [
        "#hyperparameter optimization\n",
        "params = {'learning_rate' : [0.05,0.1,0.15,0.2,0.25,0.3],\n",
        "          'max_depth' : [3,4,5,6,8,10,12,15],\n",
        "          'min_child_weight' : [1,3,5,7],\n",
        "          'gamma' : [0,0.1,0.2,0.3,0.4],\n",
        "          'colsample_bytree' : [0.3,0.4,0.5,0.7]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTYtPjT7dAQK"
      },
      "outputs": [],
      "source": [
        "xgb_regressor = xgboost.XGBRegressor()\n",
        "random_search = RandomizedSearchCV(xgb_regressor, param_distributions=params, n_iter=5, scoring='neg_mean_squared_error',n_jobs=-1,cv=5,verbose=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbnUFZ61fPyU"
      },
      "outputs": [],
      "source": [
        "random_search.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTZalABlfwfQ"
      },
      "outputs": [],
      "source": [
        "#Getting best estimator\n",
        "xgb_model = random_search.best_estimator_\n",
        "xgb_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the metric value\n",
        "metric_value(xgb_model,x_train,y_train,x_test,y_test)"
      ],
      "metadata": {
        "id": "nB7vgzU_-eSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE1Q5tdPSq5N"
      },
      "outputs": [],
      "source": [
        "#feature importance\n",
        "feature_importance(xgb_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aUaosySqcRg"
      },
      "source": [
        "* **XgBoost with RandomisedSearchCV Model**\n",
        "* **R2 score of Train data : 0.98**\n",
        "* **adjusted R2 score of Train data : 0.98**\n",
        "* **R2 score of Test data : 0.94**\n",
        "* **adjusted R2 score of Test data : 0.93**\n",
        "* **Performance of data: Best**\n",
        "* To satisfy the regression assumptions and be able to trust the results, **the residuals should have a constant variance**.\n",
        "* **Important features:** hour:1,11, month:8,12, snowfall, holiday_no_holiday, season_spring, weekdays_weekend"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Result**"
      ],
      "metadata": {
        "id": "_GJrbb-wCmbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Specify the Column Names while initializing the Table\n",
        "myTable = PrettyTable([\"Model_Name\",'R^2 of train datset',\n",
        "                       \"Adjusted R^2 of train datset\",\n",
        "                       'R^2 of test datset', \"Adjusted R^2 of test datset\"])\n",
        "\n",
        "# Add rows\n",
        "myTable.add_row([\"Linear Regression\",\"86%\",\"86%\",'86%','86%'])\n",
        "myTable.add_row([\"Ridge Regression\",\"86%\",\"86%\",'86%','86%'])\n",
        "myTable.add_row([\"Decision Tree with GridSearchCV\",\"94%\",\"93%\",'88%','88%'])\n",
        "myTable.add_row([\"Random Forest\", \"99%\",\"99%\",'93%','93%'])\n",
        "myTable.add_row([\"Gradient Boosting\", \"89%\",\"89%\",'88%','88%'])\n",
        "myTable.add_row([\"Gradient Boosting with GridSearchCV\", \"96%\",\"96%\",'93%','93%'])\n",
        "myTable.add_row([\"XgBoost Regressor with RandomisedSearchCV\", \"98%\",\"98%\",'94%','93%'])\n",
        "\n",
        "print(myTable)"
      ],
      "metadata": {
        "id": "dtvCbuBVCloc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyUG-jtKzwVV"
      },
      "source": [
        "• **No overfitting** is seen.\n",
        "\n",
        "• **Random forest Regressor, Gradient Boosting gridsearchcv, XgBoost Regressor with RandomizedSearchCV** gives the highest R2 score.\n",
        "\n",
        "• Feature Importance value for Random Forest, Gradient Boosting, XgBoost are different.\n",
        "\n",
        "• We can deploy **Random Forest, Gradient Boosting with GridSearchCV, XgBoost with RandomizedSearchCV** model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPott2RnH10D"
      },
      "source": [
        "#**Model Explainability by LIME**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK-LjCNsyRNY"
      },
      "outputs": [],
      "source": [
        "# Extract features\n",
        "float_columns=[]\n",
        "cat_columns=[]\n",
        "int_columns=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkXs6MSTGHYd"
      },
      "outputs": [],
      "source": [
        "# Putting features into respective float, cat , int list.\n",
        "for i in x.columns:\n",
        "    if x[i].dtype == 'float' :\n",
        "        float_columns.append(i)\n",
        "    elif x[i].dtype == 'int64':\n",
        "        int_columns.append(i)\n",
        "    elif x[i].dtype == 'object':\n",
        "        cat_columns.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiJm9ObfGJKa"
      },
      "outputs": [],
      "source": [
        "train_cat_features = x[cat_columns]\n",
        "train_float_features = x[float_columns]\n",
        "train_int_features = x[int_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv9TsbEOGcEA"
      },
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_fl6NLLGeZW"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "from __future__ import print_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYfWjRkaG1x4"
      },
      "outputs": [],
      "source": [
        "# Create the LIME Explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(feature_names = x.columns,\n",
        "                                                  training_data = np.array(x_train),\n",
        "                                                  mode='regression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeMPeGDdG71z"
      },
      "outputs": [],
      "source": [
        "# Get the explanation for RandomForest\n",
        "exp = explainer.explain_instance(data_row = x_test.iloc[24], predict_fn = rf_model.predict)\n",
        "exp.show_in_notebook(show_table=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c1qRh2zTfN7"
      },
      "outputs": [],
      "source": [
        "# Get the explanation for Gradient Boosting with GridSearchCV\n",
        "exp = explainer.explain_instance(data_row = x_test.iloc[24], predict_fn = gradient_boosting_gridsearchcv.predict)\n",
        "exp.show_in_notebook(show_table=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v17MDnH_V095"
      },
      "outputs": [],
      "source": [
        "# Get the explanation for XGBoost with GridSearchCV\n",
        "exp = explainer.explain_instance(data_row = x_test.iloc[24], predict_fn = xgb_model.predict)\n",
        "exp.show_in_notebook(show_table=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0aK_F7iuhqk"
      },
      "source": [
        "#**Summary and conclusions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGGpoT4wu67Y"
      },
      "source": [
        "###**EDA Summary**\n",
        "* **Count of Rented Bike(dependent variable) : positively skewed**\n",
        "* **Normally distributed attributes:**\n",
        "              - Temperature\n",
        "              - Humidity.\n",
        "* **Positively skewed attributes:**\n",
        "              - Wind\n",
        "              - Solar Radiation\n",
        "              - Snowfall\n",
        "              - Rainfall\n",
        "* **Negatively skewed attributes:**\n",
        "              - visibility\n",
        "* **positively correlated variables to the bike rent are :**\n",
        "              - Temperature\n",
        "              - Dew Point Temperature\n",
        "              - Solar Radiation\n",
        "              - Hour\n",
        "\n",
        "* **And most negatively correlated variables are:**\n",
        "              - Humidity\n",
        "              - Rainfall\n",
        "              - Weekdays or Weekends\n",
        "* The number of bikes rented is on average higher during the rush hours.i.e. at 6 p.m. to 8 p.m.\n",
        "* The **rented bike counts is higher during the summer and lowest during the winter**.\n",
        "* The **rented bike count is higher on working days than on non-working days**.\n",
        "* On a **non-functioning day, no bikes are rented** in all the instances of the data.\n",
        "* The number of bikes rented on average remains constant throughout Monday - Saturday, it dips on Sunday, and on average, the **rented bike counts is lower on weekends than on weekdays**.\n",
        "* On regular days, the demand for the bikes is higher during rush hours. **On holidays or weekends, the demand is comparatively lower in the mornings, and is higher in the afternoons**.\n",
        "\n",
        "**Conclusion after using different models:**\n",
        "\n",
        "      - No overfitting is seen.\n",
        "      - Random forest Regressor, Gradient Boosting gridsearchcv, XbBoost Regressor with GridSearchCV gives the highest R2 score .\n",
        "      - Feature Importance value for Random Forest, Gradient Boosting, XgBoost are different.\n",
        "      - We can deploy Random Forest, Gradient Boosting with  GridSearchCV, XgBoost with RandomizedSearchCV model.\n",
        "\n",
        "**Important Feature which affect the most in the rented bike count:**\n",
        "\n",
        "    * Functioning Day\n",
        "    * Winter Season\n",
        "    * 18th Hour i.e. 6 p.m.\n",
        "    * Temperature\n",
        "    * Humidity\n",
        "\n",
        "**Features which impact Negatively on Rented Bike Count:**  \n",
        "\n",
        "    * Hour 4 i.e. 4 a.m.\n",
        "    * Hour 5 i.e. 5 a.m.\n",
        "    * Hour 3 i.e. 3 a.m.\n",
        "    * Winter Season\n",
        "    * Rainfall\n",
        "    * weekdays_weekend\n",
        "\n",
        "**Features which impact Positively on Rented Bike Count:**\n",
        "\n",
        "    * Hour 18 i.e. 6 p.m.\n",
        "    * Hour 19 i.e. 7 p.m.\n",
        "    * Hour 8 i.e. 8 a.m.\n",
        "    * Hour 21 i.e. 9 p.m.\n",
        "    * Hour 20  i.e. 8 p.m.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ltdFrjO3FK3"
      },
      "source": [
        "However, this is not the ultimate end. As **this data is time dependent**, the values of features like **Temperature, Windspeed, Solar Radiation etc., will not always be consistent.** Therefore, there will be scenarios where the model might not perform well. As Machine learning is an exponentially evolving field, we will have to be prepared for all contingencies and also keep checking our model from time to time. Therefore, having a quality knowledge and keeping pace with the ever evolving ML field would surely help one to stay a step ahead in future."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "evD2QFDi_edi",
        "Esxwrhuqqyco",
        "uu5NPrVTtHNV",
        "9N_m7rYftXCs",
        "LjLVv8iztslE",
        "LaLgFDw228TU",
        "HIexTjdUFnVZ",
        "B4rmDSr6Fq4j",
        "1F3WeauuF2sf",
        "cPott2RnH10D",
        "h0aK_F7iuhqk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}